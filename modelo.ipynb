{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importación de librerías\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jeman\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\jeman\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "#Cargar el modelo preentrenado\n",
    "base_model = load_model('cifar10.h5')\n",
    "\n",
    "#Congelar capas para conservar características\n",
    "for layer in base_model.layers[:-5]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ajustar las últimas capas del modelo\n",
    "n_classes = 4\n",
    "\n",
    "x = base_model.layers[-2].output                                                #Conectar a la penúltima capa del modelo base\n",
    "x = Dense(256, activation='relu', name='custom_dense_stl10')(x)                 #Nombre único para la primera capa densa\n",
    "output = Dense(n_classes, activation='softmax', name='custom_output_stl10')(x)  #Nombre único para la capa de salida\n",
    "model = Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar datos desde el directorio\n",
    "data_dir = '4Clases/'\n",
    "all_images = []\n",
    "all_labels = []\n",
    "\n",
    "#Recorrer las clases dentro del directorio principal\n",
    "for label in range(n_classes):\n",
    "    class_dir = os.path.join(data_dir, f'class_{label}')\n",
    "    #Obtener lista de imágenes en cada clase\n",
    "    image_names = os.listdir(class_dir)\n",
    "    #Completar las rutas de las imágenes\n",
    "    image_paths = [os.path.join(class_dir, img_name) for img_name in image_names if img_name.endswith('.jpg')]\n",
    "    #Agregar a la lista de todas las imágenes y etiquetas\n",
    "    all_images.extend(image_paths)\n",
    "    all_labels.extend([label] * len(image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear conjuntos de datos\n",
    "batch_size = 32\n",
    "img_height = 32\n",
    "img_width = 32\n",
    "\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    all_images, all_labels, test_size=0.3, random_state=123)\n",
    "\n",
    "val_images, test_images, val_labels, test_labels = train_test_split(\n",
    "    test_images, test_labels, test_size=0.5, random_state=123)\n",
    "\n",
    "#Procesamiento de imágenes\n",
    "def preprocess_image(image_path, label):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, size=(img_height, img_width))\n",
    "    img = tf.cast(img, tf.float32) / 255.0  # Normalizar a [0,1]\n",
    "    return img, label\n",
    "\n",
    "#Crear datasets de entrenamiento, validación y prueba\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "train_dataset = train_dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "val_dataset = val_dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "test_dataset = test_dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#Calcular la media y la desviación estándar de los datos de entrenamiento\n",
    "mean = 0\n",
    "std = 0\n",
    "num_batches = 0\n",
    "\n",
    "for images, _ in train_dataset:\n",
    "    batch_mean = tf.reduce_mean(images, axis=[0, 1, 2])\n",
    "    batch_std = tf.math.reduce_std(images, axis=[0, 1, 2])\n",
    "    mean += batch_mean\n",
    "    std += batch_std\n",
    "    num_batches += 1\n",
    "\n",
    "mean /= num_batches\n",
    "std /= num_batches\n",
    "\n",
    "#Normalizar los datos usando z-score y codificar las etiquetas\n",
    "def normalize_and_encode_img(image, label):\n",
    "    image = (image - mean) / (std + 1e-7)\n",
    "    label = tf.one_hot(label, n_classes)\n",
    "    return image, label\n",
    "\n",
    "train_dataset = train_dataset.map(normalize_and_encode_img)\n",
    "val_dataset = val_dataset.map(normalize_and_encode_img)\n",
    "test_dataset = test_dataset.map(normalize_and_encode_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compilar el modelo\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Definir callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
    "\n",
    "#Entrenar el modelo\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=100,\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluar el modelo\n",
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "print(f\"Pérdida: {loss}\")\n",
    "print(f\"Precisión: {accuracy}\")\n",
    "\n",
    "# Predicciones y etiquetas verdaderas para el conjunto de prueba\n",
    "predictions = model.predict(test_dataset)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_true = np.concatenate([y for _, y in test_dataset], axis=0)\n",
    "y_true = np.argmax(y_true, axis=1)  # Convertir y_true a su forma original\n",
    "\n",
    "# Calcular precisión global del nuevo modelo usando sklearn\n",
    "precision_global = accuracy_score(y_true, y_pred)\n",
    "print(\"Precisión global del nuevo modelo:\", precision_global)\n",
    "\n",
    "#Guardar el modelo entrenado\n",
    "model.save('modelo.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Modelo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
